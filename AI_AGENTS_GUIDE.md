# ü§ñ AI Agents Guide - H∆∞·ªõng d·∫´n chi ti·∫øt c√°c AI Agent

## üìã T·ªïng quan AI Agents trong h·ªá th·ªëng

H·ªá th·ªëng Enhanced AI Video Search t√≠ch h·ª£p **4 lo·∫°i AI Agents** ch√≠nh ƒë·ªÉ h·ªó tr·ª£ t√¨m ki·∫øm v√† ph√¢n t√≠ch video th√¥ng minh:

---

## üß† 1. OpenAI GPT-4 Vision Agent

### **Ch·ª©c nƒÉng ch√≠nh:**
- **Ph√¢n t√≠ch h√¨nh ·∫£nh chi ti·∫øt**: M√¥ t·∫£ n·ªôi dung frame video
- **Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ h√¨nh ·∫£nh**: Visual Question Answering (VQA)
- **T·ªëi ∆∞u query t√¨m ki·∫øm**: Chuy·ªÉn ƒë·ªïi ng√¥n ng·ªØ t·ª± nhi√™n th√†nh query t·ªëi ∆∞u

### **C√°ch s·ª≠ d·ª•ng:**
```python
from ai_agent_manager import AIAgentManager, AgentConfig

# C·∫•u h√¨nh GPT-4 Vision
agent_config = AgentConfig(
    provider="openai",
    model="gpt-4-vision-preview",
    max_tokens=4000,
    temperature=0.1
)

manager = AIAgentManager()

# Ph√¢n t√≠ch frame video
result = manager.analyze_frame(
    image_path="frame_001.jpg",
    prompt="M√¥ t·∫£ chi ti·∫øt nh·ªØng g√¨ b·∫°n th·∫•y trong h√¨nh n√†y",
    config=agent_config
)

# T·ªëi ∆∞u query t√¨m ki·∫øm
optimized_query = manager.generate_search_query(
    user_query="t√¨m c·∫£nh ng∆∞·ªùi n√≥i chuy·ªán ƒëi·ªán tho·∫°i",
    config=agent_config
)
```

### **L·ª£i √≠ch:**
‚úÖ **ƒê·ªô ch√≠nh x√°c cao** - Hi·ªÉu r√µ ng·ªØ c·∫£nh v√† chi ti·∫øt  
‚úÖ **H·ªó tr·ª£ ti·∫øng Vi·ªát** - X·ª≠ l√Ω c√¢u h·ªèi ti·∫øng Vi·ªát t·ª± nhi√™n  
‚úÖ **Reasoning ability** - C√≥ kh·∫£ nƒÉng suy lu·∫≠n ph·ª©c t·∫°p  
‚úÖ **Multi-modal** - K·∫øt h·ª£p text v√† image analysis  

### **Khi n√†o s·ª≠ d·ª•ng:**
- C·∫ßn ph√¢n t√≠ch chi ti·∫øt n·ªôi dung frame
- T√¨m ki·∫øm ph·ª©c t·∫°p v·ªõi ng·ªØ c·∫£nh
- Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ n·ªôi dung video
- T·∫°o m√¥ t·∫£ t·ª± ƒë·ªông cho video

---

## üéØ 2. Anthropic Claude Agent

### **Ch·ª©c nƒÉng ch√≠nh:**
- **X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n**: Hi·ªÉu √Ω ƒë·ªãnh ng∆∞·ªùi d√πng
- **T·ªëi ∆∞u query**: Chuy·ªÉn ƒë·ªïi th√†nh search terms hi·ªáu qu·∫£
- **Ph√¢n t√≠ch ng·ªØ c·∫£nh**: Hi·ªÉu context v√† intent

### **C√°ch s·ª≠ d·ª•ng:**
```python
# C·∫•u h√¨nh Claude
agent_config = AgentConfig(
    provider="anthropic", 
    model="claude-3-sonnet-20240229",
    max_tokens=4000
)

# T·ªëi ∆∞u query t√¨m ki·∫øm th√¥ng minh
optimized_query = manager.generate_search_query(
    user_query="t√¨m nh·ªØng c·∫£nh c√≥ ng∆∞·ªùi ƒë√†n √¥ng m·∫∑c vest trong ph√≤ng h·ªçp",
    config=agent_config
)

# Ph√¢n t√≠ch √Ω ƒë·ªãnh t√¨m ki·∫øm
intent_analysis = manager.analyze_search_intent(
    query="ng∆∞·ªùi ph·ª• n·ªØ ƒëang thuy·∫øt tr√¨nh v·ªÅ d·ª± √°n",
    config=agent_config
)
```

### **L·ª£i √≠ch:**
‚úÖ **X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n t·ªët** - Hi·ªÉu c√¢u h·ªèi ph·ª©c t·∫°p  
‚úÖ **Reasoning m·∫°nh** - Ph√¢n t√≠ch √Ω ƒë·ªãnh ch√≠nh x√°c  
‚úÖ **Context awareness** - Hi·ªÉu ng·ªØ c·∫£nh t√¨m ki·∫øm  
‚úÖ **Optimization** - T·ªëi ∆∞u query cho k·∫øt qu·∫£ t·ªët nh·∫•t  

### **Khi n√†o s·ª≠ d·ª•ng:**
- Query t√¨m ki·∫øm ph·ª©c t·∫°p, nhi·ªÅu ƒëi·ªÅu ki·ªán
- C·∫ßn hi·ªÉu √Ω ƒë·ªãnh t√¨m ki·∫øm c·ªßa ng∆∞·ªùi d√πng
- T·ªëi ∆∞u performance t√¨m ki·∫øm
- X·ª≠ l√Ω c√¢u h·ªèi m∆° h·ªì

---

## üî¨ 3. Local BLIP Models Agent

### **Ch·ª©c nƒÉng ch√≠nh:**
- **Image Captioning**: T·∫°o caption t·ª± ƒë·ªông cho frame
- **Visual Question Answering**: Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ h√¨nh ·∫£nh
- **Ho·∫°t ƒë·ªông offline**: Kh√¥ng c·∫ßn API key

### **C√°ch s·ª≠ d·ª•ng:**
```python
# C·∫•u h√¨nh BLIP model (offline)
agent_config = AgentConfig(
    provider="local",
    model="Salesforce/blip-image-captioning-base"
)

# T·∫°o caption cho frame
caption = manager.analyze_frame(
    image_path="frame_002.jpg", 
    prompt="Describe this image",
    config=agent_config
)

# Visual Question Answering
answer = manager.visual_qa(
    image_path="frame_002.jpg",
    question="What is the person doing?",
    config=agent_config
)
```

### **L·ª£i √≠ch:**
‚úÖ **Ho·∫°t ƒë·ªông offline** - Kh√¥ng c·∫ßn internet ho·∫∑c API key  
‚úÖ **T·ªëc ƒë·ªô cao** - X·ª≠ l√Ω local tr√™n GPU  
‚úÖ **Privacy** - D·ªØ li·ªáu kh√¥ng g·ª≠i ra ngo√†i  
‚úÖ **Cost-effective** - Kh√¥ng t·ªën ph√≠ API  

### **Khi n√†o s·ª≠ d·ª•ng:**
- X·ª≠ l√Ω batch l·ªõn frames
- M√¥i tr∆∞·ªùng b·∫£o m·∫≠t, kh√¥ng k·∫øt n·ªëi internet
- T·∫°o metadata t·ª± ƒë·ªông cho video
- Chi ph√≠ API quan tr·ªçng

---

## üé≠ 4. Hybrid Model Manager

### **Ch·ª©c nƒÉng ch√≠nh:**
- **K·∫øt h·ª£p multiple models**: CLIP + BLIP + Vision models
- **Intelligent routing**: Ch·ªçn model ph√π h·ª£p cho t·ª´ng task
- **Performance optimization**: T·ªëi ∆∞u GPU memory v√† speed

### **C√°ch s·ª≠ d·ª•ng:**
```python
from enhanced_hybrid_manager import EnhancedHybridModelManager

manager = EnhancedHybridModelManager()

# Text-to-image search v·ªõi multiple models
results = manager.search_by_text(
    query="ng∆∞·ªùi ph·ª• n·ªØ ƒëang n√≥i chuy·ªán ƒëi·ªán tho·∫°i",
    top_k=10,
    use_models=["clip", "chinese-clip", "blip"]
)

# Image similarity v·ªõi ensemble approach
similar_frames = manager.search_by_image(
    image_path="query_frame.jpg",
    top_k=5,
    similarity_threshold=0.8,
    ensemble=True  # K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ nhi·ªÅu model
)

# Advanced search v·ªõi filters
filtered_results = manager.advanced_search(
    query="meeting room presentation",
    filters={
        "video_name": ["business_meeting.mp4"],
        "timestamp_range": (100, 500),
        "min_confidence": 0.7
    },
    use_agents=True  # S·ª≠ d·ª•ng AI agents ƒë·ªÉ optimize
)
```

### **L·ª£i √≠ch:**
‚úÖ **Best of all worlds** - K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa nhi·ªÅu model  
‚úÖ **Intelligent selection** - T·ª± ƒë·ªông ch·ªçn model t·ªëi ∆∞u  
‚úÖ **High accuracy** - Ensemble approach cho ƒë·ªô ch√≠nh x√°c cao  
‚úÖ **GPU optimized** - T·ªëi ∆∞u memory v√† performance  

---

## üéØ T√≠ch h·ª£p AI Agents v√†o workflow

### **1. Search Workflow v·ªõi AI Agents**

```python
# B∆∞·ªõc 1: User input
user_query = "t√¨m c·∫£nh c√≥ ng∆∞·ªùi ƒë√†n √¥ng ƒëang thuy·∫øt tr√¨nh"

# B∆∞·ªõc 2: Claude t·ªëi ∆∞u query
optimized_query = claude_agent.optimize_query(user_query)
# Output: "man giving presentation business meeting"

# B∆∞·ªõc 3: Hybrid manager t√¨m ki·∫øm
results = hybrid_manager.search_by_text(optimized_query, top_k=20)

# B∆∞·ªõc 4: GPT-4 Vision re-rank results
for result in results:
    confidence = gpt4_vision.analyze_relevance(
        image=result.frame_path,
        query=user_query
    )
    result.ai_confidence = confidence

# B∆∞·ªõc 5: Tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë∆∞·ª£c AI optimize
final_results = sorted(results, key=lambda x: x.ai_confidence, reverse=True)[:10]
```

### **2. Auto-tagging v·ªõi AI Agents**

```python
# T·ª± ƒë·ªông t·∫°o tags cho video m·ªõi
def auto_tag_video(video_path):
    frames = extract_key_frames(video_path)
    
    for frame in frames:
        # BLIP t·∫°o caption c∆° b·∫£n
        basic_caption = blip_agent.generate_caption(frame)
        
        # GPT-4 Vision ph√¢n t√≠ch chi ti·∫øt
        detailed_analysis = gpt4_vision.analyze_frame(
            frame, 
            prompt="Describe the scene, objects, people, and activities in detail"
        )
        
        # Claude extract keywords
        keywords = claude_agent.extract_keywords(detailed_analysis)
        
        # L∆∞u metadata
        save_frame_metadata(frame, {
            'basic_caption': basic_caption,
            'detailed_analysis': detailed_analysis,
            'keywords': keywords,
            'ai_processed': True
        })
```

---

## üí° C√°c t√¨nh hu·ªëng s·ª≠ d·ª•ng th·ª±c t·∫ø

### **üé¨ T√¨m ki·∫øm n·ªôi dung video**
**T√¨nh hu·ªëng**: T√¨m c·∫£nh c·ª• th·ªÉ trong video d√†i  
**Agents s·ª≠ d·ª•ng**: Claude (optimize query) + Hybrid Manager (search) + GPT-4 (re-rank)  
**L·ª£i √≠ch**: T√¨m ch√≠nh x√°c, hi·ªÉu ng·ªØ c·∫£nh, k·∫øt qu·∫£ relevance cao  

### **üìä Ph√¢n t√≠ch n·ªôi dung t·ª± ƒë·ªông**
**T√¨nh hu·ªëng**: T·∫°o metadata cho th∆∞ vi·ªán video l·ªõn  
**Agents s·ª≠ d·ª•ng**: BLIP (batch captioning) + GPT-4 (detailed analysis)  
**L·ª£i √≠ch**: T·ª± ƒë·ªông h√≥a, chi ph√≠ th·∫•p, quality cao  

### **üîç Visual Question Answering**
**T√¨nh hu·ªëng**: "C√≥ bao nhi√™u ng∆∞·ªùi trong c·∫£nh n√†y?"  
**Agents s·ª≠ d·ª•ng**: GPT-4 Vision (primary) + BLIP (backup)  
**L·ª£i √≠ch**: Tr·∫£ l·ªùi ch√≠nh x√°c c√¢u h·ªèi ph·ª©c t·∫°p v·ªÅ h√¨nh ·∫£nh  

### **üéØ Content Moderation**
**T√¨nh hu·ªëng**: Ki·ªÉm tra n·ªôi dung kh√¥ng ph√π h·ª£p  
**Agents s·ª≠ d·ª•ng**: Multiple models v·ªõi voting mechanism  
**L·ª£i √≠ch**: ƒê·ªô ch√≠nh x√°c cao, gi·∫£m false positive/negative  

---

## ‚öôÔ∏è C·∫•u h√¨nh v√† T·ªëi ∆∞u

### **API Keys Setup**
```bash
# T·∫°o file .env
cp .env.example .env

# Th√™m API keys
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
```

### **Performance Tuning**
```python
# T·ªëi ∆∞u cho GPU memory
config = {
    'batch_size': 16,  # Gi·∫£m n·∫øu GPU memory th·∫•p
    'mixed_precision': True,
    'cache_models': True,
    'max_concurrent_requests': 4
}

# T·ªëi ∆∞u cho cost
config = {
    'use_local_first': True,  # ∆Øu ti√™n BLIP tr∆∞·ªõc API
    'api_fallback': True,     # Ch·ªâ d√πng API khi c·∫ßn
    'cache_api_results': True # Cache ƒë·ªÉ tr√°nh duplicate calls
}
```

---

## üöÄ K·∫øt lu·∫≠n

AI Agents trong h·ªá th·ªëng gi√∫p:

1. **üéØ TƒÉng ƒë·ªô ch√≠nh x√°c t√¨m ki·∫øm** - Hi·ªÉu √Ω ƒë·ªãnh v√† ng·ªØ c·∫£nh
2. **‚ö° T·ªëi ∆∞u performance** - Intelligent routing v√† caching  
3. **üí∞ Gi·∫£m chi ph√≠** - K·∫øt h·ª£p local models v√† cloud APIs
4. **üîß T·ª± ƒë·ªông h√≥a** - Auto-tagging v√† content analysis
5. **üåç ƒêa ng√¥n ng·ªØ** - H·ªó tr·ª£ ti·∫øng Vi·ªát v√† English t·ª± nhi√™n

**Khuy·∫øn ngh·ªã**: B·∫Øt ƒë·∫ßu v·ªõi BLIP (local) cho basic features, sau ƒë√≥ th√™m Claude v√† GPT-4 Vision cho advanced capabilities.
