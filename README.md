# ğŸ¥ Enhanced Video Search System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-Latest-green.svg)
![Mode](https://img.shields.io/badge/Mode-Simple%20%26%20Fast-brightgreen.svg)

**ğŸ§  Modern video search system - Simple & Reliable**

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Features](#-features) â€¢ [ğŸ¯ Usage](#-usage) â€¢ [ğŸ”§ API](#-api-reference)

</div>

---

## ğŸ“‹ Overview

Enhanced Video Search System lÃ  há»‡ thá»‘ng tÃ¬m kiáº¿m video thÃ´ng minh vá»›i kháº£ nÄƒng **auto-detection** vÃ  **intelligent fallback**. Há»‡ thá»‘ng tá»± Ä‘á»™ng phÃ¡t hiá»‡n TensorFlow availability vÃ  chá»n mode phÃ¹ há»£p.

### âœ¨ Key Features

- ï¿½ **Auto-Detection**: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n TensorFlow Hub vÃ  chá»n mode tá»‘i Æ°u
- ï¿½ **Intelligent Fallback**: Full mode â†’ Simple mode khi TensorFlow khÃ´ng available  
- ğŸŒ **Cross-Platform**: Windows, Linux, macOS vá»›i 1 launcher thá»‘ng nháº¥t
- âš¡ **Smart Search**: Vector search vá»›i TensorFlow Hub hoáº·c keyword search
- ğŸ¯ **Unified Experience**: 1 cÃ¡ch duy nháº¥t Ä‘á»ƒ khá»Ÿi Ä‘á»™ng vÃ  sá»­ dá»¥ng
- ğŸŒ **Multiple Interfaces**: API, Web UI Ä‘á»u tÃ­ch há»£p auto-detection

---

## ğŸš€ Quick Start

### 1. One-Click Launch (Cross-Platform)

**ğŸªŸ Windows - Multiple Options:**
```cmd
# Option 1: Simple batch (recommended for beginners)
start.bat

# Option 2: Modern PowerShell (advanced features)
powershell -ExecutionPolicy Bypass -File start.ps1

# Option 3: Direct Python
python start.py
```

**ğŸ§ Linux:**
```bash
# Make executable (first time only)
chmod +x start.sh

# Launch
./start.sh

# Or direct Python
python3 start.py
```

**ğŸ macOS:**
```bash
# Make executable (first time only)
chmod +x start.sh

# Launch
./start.sh

# Or direct Python
python3 start.py
```

**ğŸŒ Universal (Any OS):**
```bash
python start.py
```

```

### 2. Smart Auto-Install Features

Launcher má»›i cÃ³ kháº£ nÄƒng **Smart Auto-Install**:

```
ğŸš€==========================================================ğŸš€
    Enhanced Video Search - Smart Auto-Install
ğŸš€==========================================================ğŸš€

ğŸ¯ CURRENT MODE: âš¡ SIMPLE MODE / ğŸ”¥ FULL MODE

CHá»ŒN CHá»¨C NÄ‚NG:
1. ğŸ§  TÃ¬m kiáº¿m video (AI-Powered) / ğŸ” Simple Mode
2. ï¿½ CÃ i Ä‘áº·t TensorFlow (Upgrade to Full Mode)
3. ï¿½ğŸ“Š Xem thÃ´ng tin há»‡ thá»‘ng
4. ğŸŒ Khá»Ÿi cháº¡y API Server
5. ğŸ› ï¸  CÃ i Ä‘áº·t dependencies
6. ï¿½ Test láº¡i TensorFlow
7. ï¿½ğŸšª ThoÃ¡t
```

**ğŸ”¥ Key Features:**
- âœ… **Auto-Detection**: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n TensorFlow cÃ³ sáºµn
- âœ… **Smart Install**: CÃ i Ä‘áº·t TensorFlow vá»›i phiÃªn báº£n tÆ°Æ¡ng thÃ­ch
- âœ… **Mode Switching**: Chuyá»ƒn Ä‘á»•i linh hoáº¡t Simple â†” Full Mode
- âœ… **Cross-Platform**: Cháº¡y mÆ°á»£t trÃªn Windows/Linux/macOS
- âœ… **Virtual Env**: Tá»± Ä‘á»™ng kÃ­ch hoáº¡t virtual environment
- âœ… **Error Handling**: Xá»­ lÃ½ lá»—i thÃ´ng minh vÃ  gá»£i Ã½ kháº¯c phá»¥c

### 3. Access Points

- **API Server**: http://localhost:8000 (chá»n option 4)
- **API Docs**: http://localhost:8000/docs
- **Search Function**: TÃ­ch há»£p trong launcher (option 1)

---

## ğŸ¯ Modern Simple Features

### ğŸš€ Simple & Fast System

Há»‡ thá»‘ng hiá»‡n táº¡i táº­p trung vÃ o:
- âœ… **Tá»‘c Ä‘á»™**: Khá»Ÿi Ä‘á»™ng nhanh, khÃ´ng dependency phá»©c táº¡p
- âœ… **á»”n Ä‘á»‹nh**: KhÃ´ng lá»—i TensorFlow compatibility 
- âœ… **Dá»… sá»­ dá»¥ng**: Menu interactive thÃ¢n thiá»‡n
- âœ… **Cross-platform**: Cháº¡y Ä‘Æ°á»£c trÃªn Windows/Linux/macOS

### ğŸŒ Unified API Server

```python
# API endpoints hiá»‡n táº¡i
GET /api/health      # Kiá»ƒm tra tráº¡ng thÃ¡i server
GET /api/search      # TÃ¬m kiáº¿m vá»›i keyword matching
GET /system/info     # ThÃ´ng tin chi tiáº¿t há»‡ thá»‘ng
```

**Simple Mode** (Current implementation):
- Keyword-based search hiá»‡u quáº£
- Metadata search trong parquet files
- Fast response time
- No complex dependencies
- Fallback functionality

---

## ğŸ“– System Architecture

1. **Quick Setup & Status Check** - Check system status and dependencies
2. **Install Dependencies** - Install required Python packages
3. **Start Enhanced API Server** - Launch FastAPI server (http://localhost:8000)
4. **Start Web Interface** - Launch Streamlit interface (http://localhost:8501)
5. **Check System Status** - Detailed system and dependency status
6. **Run Complete Setup** - Full system setup and testing

## Requirements

- Python 3.8 or higher
- Internet connection (for downloading dependencies)
- At least 2GB RAM (for TensorFlow operations)
- 1GB free disk space

## First Time Setup

1. Clone/download the project
2. Navigate to the project directory
3. Run the appropriate launcher for your OS:
   - Windows: `run.bat`
   - Linux/macOS: `./run.sh`
4. Choose option [6] for complete setup
5. Add video files to the `videos/` directory
6. Start using the system!

## Troubleshooting

### Windows
- If you get encoding errors, the launcher automatically sets UTF-8 encoding
- Run as Administrator if you have permission issues

### Linux/macOS
- Make sure the script is executable: `chmod +x run.sh`
- Use `python3` if `python` points to Python 2

### All Platforms
- Make sure you're in the project root directory
- Check that Python 3.8+ is installed
- Ensure you have internet connection for package downloads

## Manual Commands

If the launchers don't work, you can run commands manually:

```bash
# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\activate

# Activate (Linux/macOS)
source .venv/bin/activate

# Install dependencies
python scripts/install_dependencies.py

# Check status
python scripts/check_status.py

# Start API server
python src/api/app.py

# Start web interface
python -m streamlit run src/ui/enhanced_web_interface.py
```

### 3. Access Interfaces

- **ğŸŒ Enhanced Web Interface**: http://localhost:8501
- **ğŸ“¡ API Documentation**: http://localhost:8000/docs
- **ğŸ” Standard Web UI**: http://localhost:5000

---

## ğŸ¤– TensorFlow Hub Integration

### ğŸ“Š Available Models & Capabilities

Há»‡ thá»‘ng tÃ­ch há»£p nhiá»u mÃ´ hÃ¬nh TensorFlow Hub máº¡nh máº½ tá»« [TensorFlow Hub](https://tfhub.dev/), má»—i mÃ´ hÃ¬nh Ä‘áº£m nháº­n má»™t chá»©c nÄƒng cá»¥ thá»ƒ trong viá»‡c xá»­ lÃ½ vÃ  tÃ¬m kiáº¿m video:

#### ğŸ”¤ **Text Understanding Models**

##### **Universal Sentence Encoder Multilingual v3**
- **ğŸ”— Link**: https://tfhub.dev/google/universal-sentence-encoder-multilingual/3
- **ğŸ¯ Chá»©c nÄƒng**: MÃ£ hÃ³a vÄƒn báº£n Ä‘a ngÃ´n ngá»¯ thÃ nh vectors 512-dimensional
- **ğŸ’¡ á»¨ng dá»¥ng trong project**:
  - Hiá»ƒu ngá»¯ nghÄ©a cá»§a query tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh
  - Táº¡o embeddings cho subtitle vÃ  metadata
  - So sÃ¡nh semantic similarity giá»¯a query vÃ  ná»™i dung video
  - Há»— trá»£ cross-language search (tÃ¬m báº±ng tiáº¿ng Viá»‡t, káº¿t quáº£ tiáº¿ng Anh)

```python
# Example usage in project:
query = "hÆ°á»›ng dáº«n náº¥u Äƒn"  # Vietnamese query
embedding = use_model([query])
# â†’ Finds cooking tutorials in English videos
```

##### **Universal Sentence Encoder v4**
- **ğŸ”— Link**: https://tfhub.dev/google/universal-sentence-encoder/4
- **ğŸ¯ Chá»©c nÄƒng**: PhiÃªn báº£n tá»‘i Æ°u cho tiáº¿ng Anh, tá»‘c Ä‘á»™ cao
- **ğŸ’¡ á»¨ng dá»¥ng**: Fallback option cho English-only content, faster processing

#### ğŸ¬ **Video Understanding Models**

##### **MoViNet A0 Action Recognition**
- **ğŸ”— Link**: https://tfhub.dev/tensorflow/movinet/a0/base/kinetics-600/classification/3
- **ğŸ¯ Chá»©c nÄƒng**: Nháº­n diá»‡n 600 loáº¡i hÃ nh Ä‘á»™ng trong video (Kinetics-600 dataset)
- **ğŸ’¡ á»¨ng dá»¥ng trong project**:
  - PhÃ¡t hiá»‡n activities: "cooking", "programming", "teaching", "exercising"
  - Temporal action localization trong video
  - Content-based video categorization
  - Smart highlights extraction

```python
# Example: TÃ¬m video cÃ³ hÃ nh Ä‘á»™ng "cooking"
query = "cooking tutorial"
# MoViNet detects: stirring, chopping, frying actions
# â†’ Returns timestamped cooking segments
```

##### **MoViNet A2 Action Recognition**
- **ğŸ”— Link**: https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3
- **ğŸ¯ Chá»©c nÄƒng**: Version cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n, slower processing
- **ğŸ’¡ á»¨ng dá»¥ng**: High-precision action detection cho important content

#### ğŸ–¼ï¸ **Visual Feature Models**

##### **EfficientNet V2 B0**
- **ğŸ”— Link**: https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2
- **ğŸ¯ Chá»©c nÄƒng**: TrÃ­ch xuáº¥t visual features tá»« frames (1280-dim vectors)
- **ğŸ’¡ á»¨ng dá»¥ng trong project**:
  - Scene understanding: office, kitchen, outdoor, classroom
  - Visual similarity search giá»¯a cÃ¡c frames
  - Thumbnail generation vÃ  key frame selection
  - Visual content clustering

```python
# Example: TÃ¬m scenes tÆ°Æ¡ng tá»±
frame_features = efficientnet_model(frame)
# â†’ Groups similar visual scenes together
# â†’ Finds frames with similar composition/lighting
```

##### **EfficientNet V2 B3**
- **ğŸ”— Link**: https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2
- **ğŸ¯ Chá»©c nÄƒng**: Higher quality visual features (1536-dim), better accuracy
- **ğŸ’¡ á»¨ng dá»¥ng**: Fine-grained visual analysis cho detailed searches

#### ğŸ” **Object Detection Models**

##### **SSD MobileNet v2**
- **ğŸ”— Link**: https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2
- **ğŸ¯ Chá»©c nÄƒng**: Real-time object detection, 80+ object classes
- **ğŸ’¡ á»¨ng dá»¥ng trong project**:
  - Detect objects: person, laptop, book, food, car
  - Content-aware search: "videos with computers", "tutorials with books"
  - Automatic tagging vÃ  metadata enrichment
  - Smart content filtering

```python
# Example: TÃ¬m video cÃ³ laptop
detections = ssd_model(frame)
# â†’ Finds frames containing laptops
# â†’ Tags videos as "programming" or "tutorial"
```

##### **Faster R-CNN**
- **ğŸ”— Link**: https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1
- **ğŸ¯ Chá»©c nÄƒng**: High-precision object detection vá»›i bounding boxes
- **ğŸ’¡ á»¨ng dá»¥ng**: Detailed object localization cho professional analysis

### ğŸ§  **Intelligent Model Selection System**

#### **Smart Overlap Detection**

Há»‡ thá»‘ng tá»± Ä‘á»™ng phÃ¡t hiá»‡n khi cÃ¡c mÃ´ hÃ¬nh cÃ³ chá»©c nÄƒng trÃ¹ng láº·p:

```
âš ï¸  OVERLAP DETECTED:
ğŸ“ Text Encoding: USE Multilingual â†” USE v4
ğŸ¬ Action Recognition: MoViNet A0 â†” MoViNet A2  
ğŸ–¼ï¸  Visual Features: EfficientNet B0 â†” EfficientNet B3
ğŸ” Object Detection: SSD MobileNet â†” Faster R-CNN

ğŸ’¡ RECOMMENDATION: Choose based on priority
   - Speed: USE v4, MoViNet A0, EfficientNet B0, SSD MobileNet
   - Accuracy: USE Multilingual, MoViNet A2, EfficientNet B3, Faster R-CNN
   - Balance: Mix of both depending on use case
```

#### **Complementary Model Combinations**

Há»‡ thá»‘ng Ä‘á» xuáº¥t káº¿t há»£p mÃ´ hÃ¬nh bá»• trá»£:

```
âœ… COMPLEMENTARY PAIRS:
ğŸ”¤ + ğŸ¬: Text Understanding + Action Recognition
   â†’ "Find cooking tutorials" = Text query + Cooking actions

ğŸ–¼ï¸ + ğŸ”: Visual Features + Object Detection  
   â†’ Scene understanding + Specific object finding

ğŸŒ + ğŸ¯: Multilingual + Specialized models
   â†’ Vietnamese query + English content analysis
```

### ğŸ’¾ **Memory Usage & Performance**

| Model | Memory | Speed | Use Case |
|-------|---------|--------|-----------|
| USE Multilingual | ~500MB | Fast | Multilingual text search |
| USE v4 | ~300MB | Very Fast | English-only content |
| MoViNet A0 | ~100MB | Fast | Real-time action detection |
| MoViNet A2 | ~300MB | Medium | High-precision actions |
| EfficientNet B0 | ~50MB | Very Fast | Quick visual features |
| EfficientNet B3 | ~200MB | Medium | Detailed visual analysis |
| SSD MobileNet | ~100MB | Fast | Real-time object detection |
| Faster R-CNN | ~500MB | Slow | Precise object localization |

### ğŸ¯ **Practical Examples**

#### **Example 1: Cooking Tutorial Search**
```
User Intent: "TÃ¬m video hÆ°á»›ng dáº«n náº¥u Äƒn"

Selected Models:
âœ… USE Multilingual (Vietnamese query understanding)
âœ… MoViNet A0 (Detect cooking actions: stirring, chopping)
âœ… EfficientNet B0 (Kitchen scene detection)
âœ… SSD MobileNet (Detect kitchen objects: knife, pan, food)

Result: Comprehensive cooking video search vá»›i:
- Semantic understanding cá»§a "hÆ°á»›ng dáº«n náº¥u Äƒn"
- Action recognition cho cooking activities
- Visual scene matching cho kitchen environments
- Object detection cho cooking utensils
```

#### **Example 2: Programming Tutorial Analysis**
```
User Intent: "Find coding tutorials with screen recordings"

Selected Models:
âœ… USE v4 (English programming terms)
âœ… EfficientNet B3 (Screen/UI pattern recognition)  
âœ… SSD MobileNet (Detect: laptop, monitor, keyboard)

Result: Precise programming content vá»›i:
- Technical term understanding
- Code editor visual patterns
- Development environment detection
```

---

## ğŸ§® Mathematical Foundations & Academic Background

### ğŸ“š **Theoretical Foundations Behind TensorFlow Hub Models**

Má»—i mÃ´ hÃ¬nh trong há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn cÃ¡c ná»n táº£ng toÃ¡n há»c vÃ  nghiÃªn cá»©u há»c thuáº­t sÃ¢u sáº¯c. Pháº§n nÃ y giáº£i thÃ­ch cÃ¡c cÃ´ng thá»©c, thuáº­t toÃ¡n vÃ  lÃ½ thuyáº¿t Ä‘áº±ng sau má»—i mÃ´ hÃ¬nh.

#### ğŸ”¤ **Universal Sentence Encoder (USE) - Mathematical Foundation**

##### **Core Architecture: Transformer + Deep Averaging Network (DAN)**

**ğŸ“ˆ Mathematical Formula:**
```
Text Embedding = f(x) = Transformer(x) âŠ• DAN(x)

Where:
- x = input text sequence [wâ‚, wâ‚‚, ..., wâ‚™]  
- Transformer(x) = MultiHead(Q, K, V) + PositionalEncoding
- DAN(x) = ReLU(Wâ‚‚ Â· ReLU(Wâ‚ Â· average(embed(x))))
- âŠ• = element-wise combination operator
```

**ğŸ§  Academic Background:**
- **Paper**: "Universal Sentence Encoder" (Cer et al., 2018)
- **Key Innovation**: Dual-encoder architecture combining:
  - **Transformer path**: Captures complex syntactic relationships
  - **DAN path**: Efficient semantic averaging for speed
- **Training Objective**: 
  ```
  L = -log Ïƒ(cos(u, vâº)) - Î£áµ¢ log Ïƒ(-cos(u, váµ¢â»))
  
  Where:
  - u, vâº = positive sentence pair embeddings
  - váµ¢â» = negative sentence embeddings  
  - Ïƒ = sigmoid function
  - cos = cosine similarity
  ```

**ğŸŒ Multilingual Extension (USE-M):**
- **Cross-lingual Training**: Trained on parallel corpora (16 languages)
- **Shared Embedding Space**: 
  ```
  d(embed_en(s), embed_vi(t)) â‰ˆ semantic_similarity(s, t)
  
  Where s = English sentence, t = Vietnamese sentence
  ```
- **Application trong project**: Enables Vietnamese â†” English cross-lingual search

##### **Embedding Space Properties:**
```
Embedding Dimension: â„âµÂ¹Â²
Distance Metric: Cosine Similarity âˆˆ [-1, 1]
Semantic Clustering: ||embed(sâ‚) - embed(sâ‚‚)||â‚‚ âˆ semantic_distance(sâ‚, sâ‚‚)
```

#### ğŸ¬ **MoViNet (Mobile Video Networks) - Mathematical Foundation**

##### **Core Innovation: Causal 3D Convolutions with Stream Buffers**

**ğŸ“ˆ Mathematical Formula:**
```
Video Feature = MoViNet(X) = Î¨(Î¦â‚(X), Î¦â‚‚(X), ..., Î¦â‚œ(X))

Where:
- X âˆˆ â„áµ€Ë£á´´Ë£áµ‚Ë£á¶œ (T=time, H=height, W=width, C=channels)
- Î¦â‚œ = Causal3DConv + StreamBuffer + TemporalPooling
- Î¨ = Classification head with 600 Kinetics action classes
```

**ğŸ§  Academic Background:**
- **Paper**: "MoViNets: Mobile Video Networks for Efficient Video Recognition" (Kondratyuk et al., 2021)
- **Key Innovation**: 
  - **Causal Convolutions**: Only use past frames â†’ Real-time processing
  - **Stream Buffers**: Maintain temporal state across video chunks
  - **Neural Architecture Search (NAS)**: Automated architecture optimization

**âš¡ Efficiency Architecture:**
```
MoViNet Architecture:
Input â†’ [Causal3D Blocks] â†’ [Stream Buffers] â†’ [Classification Head]

Stream Buffer Update:
S_t = Î±Â·S_{t-1} + Î²Â·Features_t
Where Î±, Î² are learned parameters for temporal memory
```

**ğŸ¯ Action Recognition Math:**
- **Temporal Receptive Field**: Exponentially growing with depth
- **Classification**: Softmax over 600 Kinetics classes
  ```
  P(action = k | video) = exp(w_k Â· f_video) / Î£â±¼ exp(w_j Â· f_video)
  ```

#### ğŸ–¼ï¸ **EfficientNet V2 - Mathematical Foundation**

##### **Neural Architecture Search + Progressive Learning**

**ğŸ“ˆ Mathematical Formula:**
```
EfficientNet Scaling:
depth = Î±^Ï†
width = Î²^Ï†  
resolution = Î³^Ï†

Subject to: Î±Â·Î²Â²Â·Î³Â² â‰ˆ 2^Ï†
Where Ï† = compound scaling coefficient
```

**ğŸ§  Academic Background:**
- **Papers**: 
  - "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks" (Tan & Le, 2019)
  - "EfficientNetV2: Smaller Models and Faster Training" (Tan & Le, 2021)
- **Key Innovation**: 
  - **Compound Scaling**: Uniformly scale depth, width, resolution
  - **Progressive Learning**: Gradually increase image size during training
  - **Fused-MBConv blocks**: Improved speed-accuracy tradeoff

**âš™ï¸ MBConv Block Mathematics:**
```
MBConv(x) = x + DropPath(SE(DWConv(Expand(x))))

Where:
- Expand: 1Ã—1 conv, expansion ratio = 6
- DWConv: Depthwise 3Ã—3 or 5Ã—5 convolution  
- SE: Squeeze-and-Excitation attention
- DropPath: Stochastic depth regularization
```

**ğŸ¯ Feature Extraction trong Project:**
- **Output**: Dense feature vectors âˆˆ â„Â¹Â²â¸â° (B0) or â„Â¹âµÂ³â¶ (B3)
- **Semantic Similarity**: Cosine distance between feature vectors
- **Application**: Visual scene matching, thumbnail generation

#### ğŸ” **Object Detection Models - Mathematical Foundation**

##### **SSD (Single Shot MultiBox Detector)**

**ğŸ“ˆ Mathematical Formula:**
```
SSD Loss = L_conf + Î±Â·L_loc

Confidence Loss:
L_conf = -Î£áµ¢âˆˆPos x_{ij}^p log(Ä‰áµ¢^p) - Î£áµ¢âˆˆNeg log(Ä‰áµ¢â°)

Localization Loss:  
L_loc = Î£áµ¢âˆˆPos Î£â‚˜âˆˆ{cx,cy,w,h} x_{ij}^m SmoothL1(láµ¢áµ - Äâ±¼áµ)

Where:
- x_{ij}^p = {1 if box i matches object j of class p, 0 otherwise}
- Ä‰áµ¢^p = predicted confidence for class p in box i
- Î± = weight balancing term (typically Î± = 1)
```

**ğŸ§  Academic Background:**
- **Paper**: "SSD: Single Shot MultiBox Detector" (Liu et al., 2016)
- **Key Innovation**: Multi-scale feature maps for different object sizes
- **Architecture**: 
  ```
  Input Image â†’ VGG16 Base â†’ Feature Pyramid â†’ [Classification + Regression] Heads
  ```

##### **Faster R-CNN - Two-Stage Detection**

**ğŸ“ˆ Mathematical Formula:**
```
Two-Stage Process:
Stage 1: RPN(x) â†’ {proposals, objectness_scores}
Stage 2: R-CNN(proposals) â†’ {classes, refined_boxes}

RPN Loss:
L_RPN = L_cls + Î»Â·L_reg
Where L_cls = binary classification (object/background)
      L_reg = bounding box regression loss
```

**ğŸ§  Academic Background:**
- **Paper**: "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks" (Ren et al., 2017)
- **Innovation**: End-to-end trainable region proposal network
- **Advantage**: Higher precision for complex scenes

### ğŸ”¬ **Information Theory & Embedding Space Analysis**

#### **Semantic Embedding Metrics**

**ğŸ“Š Cosine Similarity Distribution:**
```
sim(u, v) = (u Â· v) / (||u|| Â· ||v||)

Semantic Properties:
- Synonyms: sim âˆˆ [0.7, 1.0]
- Related concepts: sim âˆˆ [0.4, 0.7]  
- Unrelated: sim âˆˆ [-0.2, 0.4]
- Antonyms: sim âˆˆ [-1.0, -0.2]
```

**ğŸ“ˆ Entropy Analysis trong Video Search:**
```
Information Gain = H(Results) - H(Results|Query)

Where:
H(Results) = -Î£áµ¢ P(video_i) log P(video_i)
H(Results|Query) = -Î£áµ¢ P(video_i|query) log P(video_i|query)
```

#### **Multi-modal Fusion Mathematics**

**ğŸ”— Feature Fusion Strategy:**
```
Combined_Score = wâ‚Â·Text_Similarity + wâ‚‚Â·Visual_Similarity + wâ‚ƒÂ·Action_Similarity

Subject to: wâ‚ + wâ‚‚ + wâ‚ƒ = 1, wáµ¢ â‰¥ 0

Optimization:
w* = argmax Î£â‚– log P(relevant_k | combined_score_k)
```

**ğŸ¯ Attention Mechanism trong Multi-modal Search:**
```
Attention Weights:
Î±áµ¢ = exp(score_i) / Î£â±¼ exp(score_j)

Final Representation:
r = Î£áµ¢ Î±áµ¢ Â· feature_i

Where score_i = query_embedding Â· feature_i
```

### ğŸ“Š **Computational Complexity Analysis**

| Model | Time Complexity | Space Complexity | FLOPs |
|-------|----------------|------------------|-------|
| **USE Multilingual** | O(n log n) | O(dÂ·n) | ~1.2B per sentence |
| **MoViNet A0** | O(TÂ·HÂ·WÂ·C) | O(buffer_size) | ~2.7B per video chunk |
| **EfficientNet B0** | O(HÂ·WÂ·CÂ·D) | O(D) | ~0.39B per image |
| **SSD MobileNet** | O(NÂ·KÂ·A) | O(NÂ·A) | ~1.2B per frame |

**Legend:**
- n = sequence length, d = embedding dimension
- T = temporal frames, HÃ—W = spatial resolution, C = channels
- N = number of boxes, K = number of classes, A = number of anchors
- D = network depth

### ğŸ§ª **Experimental Validation & Benchmarks**

#### **Performance Metrics trong Academic Literature:**

**Text Understanding (USE):**
- **STS Benchmark**: Pearson correlation = 0.78-0.82
- **Cross-lingual Transfer**: BLEU score improvement = +15-20%

**Action Recognition (MoViNet):**
- **Kinetics-600**: Top-1 Accuracy = 72.8% (A0), 76.5% (A2)
- **Real-time Performance**: 54 FPS (A0), 28 FPS (A2) on mobile GPU

**Visual Features (EfficientNet):**
- **ImageNet Top-1**: 77.1% (B0), 82.0% (B3)
- **Transfer Learning**: +5-10% accuracy on downstream tasks

**Object Detection:**
- **COCO mAP**: 22.2 (SSD MobileNet), 37.4 (Faster R-CNN)
- **Inference Speed**: 22ms vs 89ms per frame

### ğŸ”„ **Mathematical Optimization trong Project**

#### **Memory-Accuracy Tradeoffs:**
```
Optimization Problem:
maximize: Î£áµ¢ wáµ¢ Â· accuracy_i
subject to: Î£áµ¢ memory_i â‰¤ M_max
           Î£áµ¢ latency_i â‰¤ L_max
           wáµ¢ âˆˆ {0, 1} (binary selection)

Solution: Dynamic programming with Lagrange multipliers
```

#### **Multi-objective Model Selection:**
```
Pareto Optimal Solutions:
fâ‚(x) = accuracy(x)  (maximize)
fâ‚‚(x) = memory(x)    (minimize)  
fâ‚ƒ(x) = latency(x)   (minimize)

Non-dominated solutions form the efficient frontier
```

### ğŸ¯ **Practical Mathematical Applications trong Video Search**

#### **Semantic Search Score Calculation:**
```python
def compute_semantic_score(query_embedding, video_features):
    """
    Compute weighted semantic similarity score
    
    Mathematical foundation:
    score = Î£áµ¢ wáµ¢ Â· cos_sim(query, feature_i) Â· confidence_i
    """
    text_sim = cosine_similarity(query_embedding, video_features['text'])
    visual_sim = cosine_similarity(query_embedding, video_features['visual'])
    action_sim = cosine_similarity(query_embedding, video_features['action'])
    
    # Learned weights from validation data
    w_text, w_visual, w_action = 0.5, 0.3, 0.2
    
    combined_score = (w_text * text_sim + 
                     w_visual * visual_sim + 
                     w_action * action_sim)
    
    return combined_score
```

#### **Temporal Action Localization Algorithm:**
```python
def temporal_action_localization(video_features, action_query, threshold=0.7):
    """
    Mathematical approach: Sliding window with exponential smoothing
    
    Smoothing formula:
    S_t = Î±Â·S_{t-1} + (1-Î±)Â·raw_score_t
    
    Peak detection:
    peaks = {t : S_t > threshold AND S_t > S_{t-1} AND S_t > S_{t+1}}
    """
    smoothed_scores = []
    alpha = 0.3  # smoothing parameter
    
    for t, features in enumerate(video_features):
        raw_score = cosine_similarity(action_query, features)
        if t == 0:
            smoothed_score = raw_score
        else:
            smoothed_score = alpha * smoothed_scores[-1] + (1-alpha) * raw_score
        smoothed_scores.append(smoothed_score)
    
    # Find peaks above threshold
    action_segments = find_peaks(smoothed_scores, threshold)
    return action_segments
```

#### **Cross-lingual Embedding Alignment:**
```python
def cross_lingual_search(vietnamese_query, english_content_embeddings):
    """
    Mathematical foundation: Shared multilingual embedding space
    
    Property: d(embed_vi(q), embed_en(c)) â‰ˆ semantic_distance(q, c)
    Where d = cosine distance, q = Vietnamese query, c = English content
    """
    # USE Multilingual maps both languages to same space
    vi_embedding = use_multilingual.encode([vietnamese_query])
    
    # Direct comparison in shared space
    similarities = cosine_similarity(vi_embedding, english_content_embeddings)
    
    # Ranking with confidence adjustment
    ranked_results = sorted(enumerate(similarities), 
                           key=lambda x: x[1], reverse=True)
    
    return ranked_results
```

### ğŸ“ˆ **Advanced Mathematical Techniques**

#### **Graph-based Video Similarity:**
```
Video Similarity Graph Construction:
G = (V, E) where V = {videos}, E = {(vâ‚, vâ‚‚) : sim(vâ‚, vâ‚‚) > Ï„}

PageRank-style Ranking:
R(v) = (1-d)/N + d Â· Î£áµ¤âˆˆneighbors(v) R(u)/|out_links(u)|

Where:
- d = damping factor (0.85)
- N = total number of videos
- R(v) = importance score of video v
```

#### **Attention-based Feature Fusion:**
```
Multi-head Attention for Feature Fusion:

Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V

Where:
- Q = query features (user intent)
- K = key features (video content)  
- V = value features (video representations)
- d_k = key dimension for scaling

Multi-modal Application:
Q_text = W_q Â· text_embedding
K_video = [W_k1Â·visual_features, W_k2Â·audio_features, W_k3Â·action_features]
V_video = [W_v1Â·visual_features, W_v2Â·audio_features, W_v3Â·action_features]
```

#### **Bayesian Model Selection:**
```
Bayesian Model Evidence:
P(M_i | Data) = P(Data | M_i) Â· P(M_i) / P(Data)

For model selection:
log P(M_i | Data) = log P(Data | M_i) + log P(M_i) - log P(Data)

Practical implementation:
- P(M_i) = prior belief about model i
- P(Data | M_i) = likelihood of data given model i
- Choose model with highest posterior probability
```

### ğŸ” **Information Retrieval Theory Applications**

#### **TF-IDF for Video Content:**
```
Extended TF-IDF for Multi-modal Content:

TF-IDF_multimodal = wâ‚Â·TF-IDF_text + wâ‚‚Â·TF-IDF_visual + wâ‚ƒÂ·TF-IDF_action

Where:
TF(t,d) = count(t,d) / total_terms(d)
IDF(t) = log(N / |{d : t âˆˆ d}|)

Visual Terms: Quantized visual features
Action Terms: Detected action categories
Text Terms: Subtitle and metadata words
```

#### **Learning to Rank (LTR) for Video Search:**
```
RankNet Loss Function:
L = Î£áµ¢â±¼ C_ij Â· log(1 + exp(-(s_i - s_j)))

Where:
- C_ij = 1 if document i should rank higher than j, 0 otherwise
- s_i, s_j = model scores for documents i, j
- Optimization: Gradient descent on neural ranking model
```

### ğŸ§® **Statistical Learning Theory**

#### **Generalization Bounds:**
```
PAC-Bayes Bound for Model Selection:
With probability â‰¥ 1-Î´:

R(h) â‰¤ RÌ‚(h) + âˆš[(KL(P||Pâ‚€) + log(2m/Î´)) / (2m-1)]

Where:
- R(h) = true risk
- RÌ‚(h) = empirical risk  
- KL(P||Pâ‚€) = KL divergence between posterior and prior
- m = number of training samples
- Î´ = confidence parameter
```

#### **Active Learning for Video Annotation:**
```
Uncertainty Sampling:
x* = argmax H(p(y|x, Î¸))

Where:
H(p) = -Î£áµ¢ p(yáµ¢) log p(yáµ¢)  (entropy)

Query by Committee:
x* = argmax Variance_Î¸ [p(y|x, Î¸)]

Application: Select most informative videos for manual annotation
```

---

## ğŸ’» Usage

### ğŸŒ Web Interface Usage

1. **Launch Enhanced Interface**:
   ```bash
   streamlit run enhanced_web_interface.py
   ```

2. **Model Selection Process**:
   - Describe your intent: "Find action sequences in cooking videos"
   - System analyzes vÃ  recommends optimal models
   - Review overlaps vÃ  select preferred configuration
   - Load models (may take 5-10 minutes first time)

3. **Video Processing**:
   - Upload or specify video path
   - Enter search query (optional)
   - Process vá»›i selected models
   - View detailed results vÃ  features

### ğŸ“¡ API Usage

#### **Intelligent Model Analysis**
```python
import requests

# Analyze requirements
response = requests.post('http://localhost:8000/analyze_models', json={
    "user_intent": "Find action sequences in cooking videos",
    "max_memory_mb": 2000,
    "processing_priority": "balanced"
})

recommendations = response.json()
print(recommendations['suggested_models'])
# â†’ ['use_multilingual', 'movinet_a0', 'efficientnet_v2_b0', 'ssd_mobilenet']
```

#### **Load Selected Models**
```python
# Load models
response = requests.post('http://localhost:8000/load_selected_models', 
    json=['use_multilingual', 'movinet_a0', 'efficientnet_v2_b0'])

print(response.json()['message'])
# â†’ "Loaded 3/3 models successfully"
```

#### **Process Video**
```python
# Process video vá»›i loaded models
response = requests.post('http://localhost:8000/process_video', json={
    "video_path": "videos/cooking_tutorial.mp4",
    "query": "chopping vegetables",
    "selected_models": ["use_multilingual", "movinet_a0"]
})

results = response.json()
print(f"Processing time: {results['processing_time']}")
print(f"Results: {results['processing_results']}")
```

### ğŸ–¥ï¸ CLI Demo Usage

```bash
python enhanced_video_demo.py

# Interactive options:
# 1. Model Analysis Demo (Quick preview)
# 2. Interactive Model Selection (Full workflow)  
# 3. Model Status Overview
```

---

## ğŸ”§ API Reference

### Core Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/analyze_models` | POST | Analyze user intent & suggest models |
| `/load_selected_models` | POST | Load specified TensorFlow Hub models |
| `/process_video` | POST | Process video vá»›i selected models |
| `/enhanced_search` | GET | Advanced search vá»›i TF Hub integration |
| `/status` | GET | System status & loaded models |

### Enhanced Endpoints

```python
# Model analysis request
{
    "user_intent": "string",
    "max_memory_mb": 2000,
    "processing_priority": "lightweight|balanced|high_accuracy"
}

# Model analysis response  
{
    "recommendations": {
        "lightweight": ["model1", "model2"],
        "balanced": ["model1", "model3"],
        "high_accuracy": ["model2", "model4"]
    },
    "overlaps_detected": {
        "balanced": {"model1": ["model2"]}
    },
    "suggested_models": ["model1", "model3"],
    "estimated_memory_usage": "~650MB"
}
```

---

## ğŸ“ Project Structure

```
enhanced-video-search/
â”œâ”€â”€ ï¿½ launch.bat                  # Quick launch menu
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“– docs/
â”‚   â”œâ”€â”€ README.md                  # Comprehensive documentation
â”‚   â”œâ”€â”€ QUICK_START.md            # 5-minute setup guide
â”‚   â””â”€â”€ CHANGELOG.md              # Version history
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ§  src/
â”‚   â”œâ”€â”€ ğŸ¤– core/
â”‚   â”‚   â””â”€â”€ enhanced_video_processor.py  # Smart model manager
â”‚   â”œâ”€â”€ ğŸ“¡ api/
â”‚   â”‚   â”œâ”€â”€ app.py                # Enhanced API vá»›i TF Hub
â”‚   â”‚   â”œâ”€â”€ simple_enhanced_api.py # Lightweight fallback
â”‚   â”‚   â””â”€â”€ vietnamese_translator.py # Vietnamese support
â”‚   â””â”€â”€ ğŸŒ ui/
â”‚       â”œâ”€â”€ enhanced_web_interface.py # Advanced Streamlit UI
â”‚       â””â”€â”€ web_search_app.py     # Standard web UI
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ¬ demos/
â”‚   â”œâ”€â”€ enhanced_video_demo.py    # Interactive TF Hub demo
â”‚   â””â”€â”€ interactive_search_demo.py # CLI demo
â”œâ”€â”€ 
â”œâ”€â”€ âš™ï¸ scripts/
â”‚   â”œâ”€â”€ setup_complete.py         # One-click setup script
â”‚   â”œâ”€â”€ start_server.bat         # Detailed startup menu
â”‚   â”œâ”€â”€ start_server_advanced.bat # Advanced startup
â”‚   â””â”€â”€ start_server_simple.bat  # Simple startup
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“¦ configs/
â”‚   â”œâ”€â”€ requirements.txt         # Core dependencies
â”‚   â””â”€â”€ requirements_enhanced.txt # TensorFlow Hub dependencies
â”œâ”€â”€ 
â””â”€â”€ ï¿½ index/, ğŸ¥ videos/, ğŸ–¼ï¸ frames/ # Data directories
```

---

## ğŸ› ï¸ Configuration Options

### Memory Configurations

| Configuration | Memory Usage | Models Included | Best For |
|---------------|-------------|-----------------|----------|
| **Lightweight** | ~650MB | USE v4, MoViNet A0, EfficientNet B0, SSD MobileNet | Fast processing, limited resources |
| **Balanced** | ~1200MB | USE Multilingual, MoViNet A0, EfficientNet B0, SSD MobileNet | Good performance/memory ratio |
| **High Accuracy** | ~2000MB+ | USE Multilingual, MoViNet A2, EfficientNet B3, Faster R-CNN | Best quality, high-end systems |

### Processing Priorities

- **Speed**: Optimize for fast results
- **Accuracy**: Optimize for best quality  
- **Balanced**: Balance speed vÃ  accuracy
- **Custom**: Manual model selection

---

## ğŸš¨ Troubleshooting

### Common Issues

1. **TensorFlow Hub models failing to load**:
   ```bash
   pip install --upgrade tensorflow tensorflow-hub tensorflow-text
   ```

2. **Memory errors**:
   - Use Lightweight configuration
   - Reduce max_memory_mb parameter
   - Close other applications

3. **Slow processing**:
   - Check GPU availability: `nvidia-smi`
   - Use GPU-optimized TensorFlow: `pip install tensorflow-gpu`

4. **Model compatibility issues**:
   - Update to latest model versions
   - Check TensorFlow version compatibility

### Performance Tips

- **First run**: Models download automatically (may take 10+ minutes)
- **GPU acceleration**: Install CUDA for faster processing
- **Memory optimization**: Use model caching vÃ  lazy loading
- **Batch processing**: Process multiple videos together

---

## ğŸ“š Academic References & Theoretical Sources

### ğŸ“– **Core Papers & Research**

#### **Fundamental Text Representation:**
1. **Cer, D., et al.** (2018). "Universal Sentence Encoder." *arXiv:1803.11175*
   - ğŸ”— https://arxiv.org/abs/1803.11175
   - **Contribution**: Dual-encoder architecture, multilingual embeddings
   - **Mathematical Innovation**: DAN + Transformer fusion

2. **Yang, Y., et al.** (2019). "Multilingual Universal Sentence Encoder for Semantic Retrieval." *ACL 2019*
   - ğŸ”— https://arxiv.org/abs/1907.04307  
   - **Contribution**: Cross-lingual transfer learning
   - **Mathematical Foundation**: Shared embedding space across languages

#### **Video Understanding & Action Recognition:**
3. **Kondratyuk, D., et al.** (2021). "MoViNets: Mobile Video Networks for Efficient Video Recognition." *CVPR 2021*
   - ğŸ”— https://arxiv.org/abs/2103.11511
   - **Innovation**: Causal 3D convolutions, stream buffers
   - **Mathematical Core**: Temporal modeling with mobile efficiency

4. **Carreira, J., Zisserman, A.** (2017). "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset." *CVPR 2017*
   - ğŸ”— https://arxiv.org/abs/1705.07750
   - **Foundation**: 3D CNNs for video understanding
   - **Dataset**: Kinetics-600 action categories

#### **Efficient Neural Architecture:**
5. **Tan, M., Le, Q.** (2019). "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks." *ICML 2019*
   - ğŸ”— https://arxiv.org/abs/1905.11946
   - **Key Insight**: Compound scaling (depth Ã— width Ã— resolution)
   - **Mathematical Formula**: Î±Â·Î²Â²Â·Î³Â² â‰ˆ 2^Ï†

6. **Tan, M., Le, Q.** (2021). "EfficientNetV2: Smaller Models and Faster Training." *ICML 2021*
   - ğŸ”— https://arxiv.org/abs/2104.00298
   - **Improvements**: Progressive learning, Fused-MBConv blocks

#### **Object Detection Foundations:**
7. **Liu, W., et al.** (2016). "SSD: Single Shot MultiBox Detector." *ECCV 2016*
   - ğŸ”— https://arxiv.org/abs/1512.02325
   - **Innovation**: Multi-scale feature maps, single-shot detection
   - **Mathematical Core**: Combined classification + localization loss

8. **Ren, S., et al.** (2017). "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks." *NIPS 2015*
   - ğŸ”— https://arxiv.org/abs/1506.01497
   - **Breakthrough**: End-to-end trainable region proposals
   - **Two-stage Architecture**: RPN + Fast R-CNN

### ğŸ§  **Theoretical Foundations**

#### **Information Theory & Embedding Spaces:**
9. **Mikolov, T., et al.** (2013). "Distributed Representations of Words and Phrases and their Compositionality." *NIPS 2013*
   - **Foundation**: Word embedding principles
   - **Mathematical**: Skip-gram with negative sampling

10. **Pennington, J., et al.** (2014). "GloVe: Global Vectors for Word Representation." *EMNLP 2014*
    - **Innovation**: Global matrix factorization for embeddings
    - **Mathematical**: Co-occurrence matrix optimization

#### **Attention & Transformer Mechanisms:**
11. **Vaswani, A., et al.** (2017). "Attention is All You Need." *NIPS 2017*
    - ğŸ”— https://arxiv.org/abs/1706.03762
    - **Revolutionary**: Self-attention mechanism
    - **Mathematical**: Scaled dot-product attention

12. **Devlin, J., et al.** (2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." *NAACL 2019*
    - ğŸ”— https://arxiv.org/abs/1810.04805
    - **Impact**: Bidirectional context understanding
    - **Foundation**: Masked language modeling

#### **Multi-modal Learning:**
13. **Radford, A., et al.** (2021). "Learning Transferable Visual Representations from Natural Language Supervision." *ICML 2021*
    - ğŸ”— https://arxiv.org/abs/2103.00020
    - **CLIP Innovation**: Vision-language contrastive learning
    - **Mathematical**: Contrastive loss in joint embedding space

14. **Jia, C., et al.** (2021). "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision." *ICML 2021*
    - ğŸ”— https://arxiv.org/abs/2102.05918
    - **ALIGN Contribution**: Large-scale noisy supervision

### ğŸ“Š **Optimization & Learning Theory**

#### **Neural Architecture Search:**
15. **Zoph, B., Le, Q.V.** (2017). "Neural Architecture Search with Reinforcement Learning." *ICLR 2017*
    - **Foundation**: Automated architecture design
    - **Mathematical**: RL-based search in architecture space

16. **Tan, M., et al.** (2019). "MnasNet: Platform-Aware Neural Architecture Search for Mobile." *CVPR 2019*
    - **Mobile Focus**: Efficiency-accuracy tradeoffs
    - **Multi-objective**: Latency-aware optimization

#### **Information Retrieval & Ranking:**
17. **Burges, C., et al.** (2005). "Learning to Rank using Gradient Descent." *ICML 2005*
    - **RankNet**: Pairwise ranking with neural networks
    - **Mathematical**: Probability-based ranking loss

18. **Liu, T.Y.** (2009). "Learning to Rank for Information Retrieval." *Foundations and Trends in Information Retrieval*
    - **Comprehensive**: Survey of ranking algorithms
    - **Theory**: Statistical learning for ranking

### ğŸ” **Mathematical Background References**

#### **Linear Algebra & Optimization:**
19. **Boyd, S., Vandenberghe, L.** (2004). "Convex Optimization." *Cambridge University Press*
    - **Foundation**: Optimization theory used in model training
    - **Applications**: Loss function optimization, constraint handling

20. **Golub, G.H., Van Loan, C.F.** (2012). "Matrix Computations." *4th Edition, Johns Hopkins University Press*
    - **Core**: Matrix operations in neural networks
    - **Numerical**: SVD, eigendecomposition for embeddings

#### **Probability & Statistics:**
21. **Bishop, C.M.** (2006). "Pattern Recognition and Machine Learning." *Springer*
    - **Comprehensive**: Bayesian methods, probabilistic models
    - **Relevance**: Model selection, uncertainty quantification

22. **Hastie, T., et al.** (2009). "The Elements of Statistical Learning." *2nd Edition, Springer*
    - **Statistical**: Learning theory foundations
    - **Mathematical**: Generalization bounds, regularization

### ğŸ¯ **Implementation & Engineering References**

#### **TensorFlow & Deep Learning Frameworks:**
23. **Abadi, M., et al.** (2016). "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems." *OSDI 2016*
    - **Framework**: Computational graph optimization
    - **Engineering**: Distributed training, GPU acceleration

24. **Bisong, E.** (2019). "Building Machine Learning and Deep Learning Models on Google Cloud Platform." *Apress*
    - **Practical**: TensorFlow Hub integration patterns
    - **Cloud**: Scalable model deployment

#### **Video Processing & Computer Vision:**
25. **Szeliski, R.** (2010). "Computer Vision: Algorithms and Applications." *Springer*
    - **Foundation**: Image processing mathematics
    - **Applications**: Feature extraction, object detection

26. **Goodfellow, I., et al.** (2016). "Deep Learning." *MIT Press*
    - **Comprehensive**: Neural network theory
    - **Mathematical**: Backpropagation, optimization landscapes

### ğŸ“ˆ **Evaluation Metrics & Benchmarks**

#### **Standard Datasets & Metrics:**
- **ImageNet**: Visual recognition benchmark (Russakovsky et al., 2015)
- **Kinetics**: Action recognition dataset (Kay et al., 2017)  
- **COCO**: Object detection benchmark (Lin et al., 2014)
- **STS Benchmark**: Semantic textual similarity (Cer et al., 2017)

#### **Performance Evaluation:**
- **mAP**: Mean Average Precision for object detection
- **Top-k Accuracy**: Classification performance metrics
- **BLEU/ROUGE**: Text generation quality
- **Cosine Similarity**: Embedding space evaluation

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- **TensorFlow Hub**: For providing powerful pre-trained models
- **Google Research**: Universal Sentence Encoder vÃ  EfficientNet models
- **DeepMind**: MoViNet action recognition models
- **OpenAI**: For AI development inspiration
- **Streamlit**: For beautiful web interface framework

---

<div align="center">

**ğŸ¥ Enhanced Video Search System - Powered by TensorFlow Hub & AI**

Made with â¤ï¸ for the AI community

</div>
