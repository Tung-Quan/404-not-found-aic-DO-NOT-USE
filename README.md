# ğŸ§  AI-Powered Multimodal Search System
## Há»‡ thá»‘ng TÃ¬m kiáº¿m Äa phÆ°Æ¡ng thá»©c Dá»±a trÃªn TrÃ­ tuá»‡ NhÃ¢n táº¡o

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.12+-orange.svg)](https://tensorflow.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Tá»•ng quan Há»‡ thá»‘ng

Há»‡ thá»‘ng tÃ¬m kiáº¿m Ä‘a phÆ°Æ¡ng thá»©c tiÃªn tiáº¿n káº¿t há»£p cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u hiá»‡n Ä‘áº¡i Ä‘á»ƒ thá»±c hiá»‡n tÃ¬m kiáº¿m ngá»¯ nghÄ©a trÃªn video, hÃ¬nh áº£nh vÃ  vÄƒn báº£n. Há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn ná»n táº£ng kiáº¿n trÃºc hybrid vá»›i kháº£ nÄƒng xá»­ lÃ½ truy váº¥n phá»©c táº¡p vÃ  tÃ¡i xáº¿p háº¡ng thÃ´ng minh.

### ğŸ¯ TÃ­nh nÄƒng ChÃ­nh

- **ğŸ” TÃ¬m kiáº¿m Cross-modal**: TÃ¬m kiáº¿m báº±ng vÄƒn báº£n trÃªn video/hÃ¬nh áº£nh
- **ğŸ§  Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn**: Hiá»ƒu truy váº¥n phá»©c táº¡p vÃ  dÃ i dÃ²ng
- **âš¡ TÃ¡i xáº¿p háº¡ng Neural**: Sá»­ dá»¥ng TensorFlow Ä‘á»ƒ tá»‘i Æ°u káº¿t quáº£
- **ğŸ¨ Giao diá»‡n Web hiá»‡n Ä‘áº¡i**: FastAPI + HTML5/CSS3/JavaScript
- **ğŸ“Š PhÃ¢n tÃ­ch Video**: TrÃ­ch xuáº¥t vÃ  tÃ¬m kiáº¿m keyframes
- **ğŸš€ Kiáº¿n trÃºc Má»Ÿ rá»™ng**: Há»— trá»£ nhiá»u mÃ´ hÃ¬nh AI Ä‘á»“ng thá»i

---

## ğŸ§® Ná»n táº£ng ToÃ¡n há»c vÃ  LÃ½ thuyáº¿t

### 1. KhÃ´ng gian Biá»ƒu diá»…n Äa phÆ°Æ¡ng thá»©c (Multimodal Representation Space)

Há»‡ thá»‘ng sá»­ dá»¥ng Ã¡nh xáº¡ tá»« cÃ¡c khÃ´ng gian Ä‘áº·c trÆ°ng khÃ¡c nhau vÃ o má»™t khÃ´ng gian chung:

```
Î¦: {V, T, I} â†’ â„áµˆ
```

Trong Ä‘Ã³:
- **V**: KhÃ´ng gian video (temporal sequences)
- **T**: KhÃ´ng gian vÄƒn báº£n (token embeddings)  
- **I**: KhÃ´ng gian hÃ¬nh áº£nh (pixel representations)
- **d**: Chiá»u cá»§a khÃ´ng gian biá»ƒu diá»…n chung (thÆ°á»ng 512 hoáº·c 768)

### 2. HÃ m TÆ°Æ¡ng Ä‘á»“ng Cosine (Cosine Similarity)

Äá»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a query vÃ  document Ä‘Æ°á»£c tÃ­nh báº±ng:

```
sim(q, d) = (q Â· d) / (||q|| Ã— ||d||)
```

Vá»›i:
- **q âˆˆ â„áµˆ**: Vector embedding cá»§a query
- **d âˆˆ â„áµˆ**: Vector embedding cá»§a document
- **||Â·||**: L2 norm

### 3. MÃ´ hÃ¬nh Attention Ä‘a cáº¥p (Multi-level Attention)

Cho sequence input **X = {xâ‚, xâ‚‚, ..., xâ‚™}**, attention weights Ä‘Æ°á»£c tÃ­nh:

```
Î±_i = softmax(W_qÂ·q^T Â· W_kÂ·x_i / âˆšd_k)
```

Output attention:
```
y = Î£áµ¢ Î±áµ¢ Â· (W_vÂ·xáµ¢)
```

### 4. HÃ m Loss cho Cross-modal Learning

```
L = L_intra + Î»â‚Â·L_inter + Î»â‚‚Â·L_ranking
```

Trong Ä‘Ã³:
- **L_intra**: Loss trong cÃ¹ng modality
- **L_inter**: Loss giá»¯a cÃ¡c modalities
- **L_ranking**: Ranking loss cho retrieval
- **Î»â‚, Î»â‚‚**: Hyperparameters cÃ¢n báº±ng

---

## ğŸ”¬ Kiáº¿n trÃºc Neural Networks

### 1. BLIP-2 (Bootstrapped Language-Image Pre-training v2)

**Kiáº¿n trÃºc 3 giai Ä‘oáº¡n:**

```
[Vision Encoder] â†’ [Q-Former] â†’ [Language Model]
     â†“              â†“              â†“
  ViT-L/14     Transformer    OPT/T5
   (304M)        (188M)       (775M+)
```

**Q-Former Architecture:**
- **32 learnable queries** Ä‘á»ƒ káº¿t ná»‘i vision vÃ  language
- **Cross-attention layers** Ä‘á»ƒ aggregate visual features
- **Self-attention** Ä‘á»ƒ model inter-query relationships

### 2. TensorFlow Reranking Network

**Kiáº¿n trÃºc Deep Neural Network:**

```
Input: [query_emb, doc_emb, similarity_features]
    â†“
Dense(512) + ReLU + Dropout(0.3)
    â†“  
Dense(256) + ReLU + Dropout(0.2)
    â†“
Dense(128) + ReLU + Dropout(0.1)
    â†“
Dense(1) + Sigmoid
    â†“
Score âˆˆ [0,1]
```

### 3. Complex Query Processor

**Intent Classification:**
```
P(intent|query) = softmax(WÂ·BERT(query) + b)
```

**Entity Extraction sá»­ dá»¥ng Named Entity Recognition:**
- Transformer-based NER vá»›i BILOU tagging scheme
- CRF layer cho sequence labeling consistency

---

## ğŸ§ª Thuáº­t toÃ¡n TÃ¬m kiáº¿m

### 1. Hybrid Search Algorithm

```python
def hybrid_search(query, Î±=0.7, Î²=0.3):
    # Semantic search
    semantic_scores = cosine_similarity(
        encode_text(query), 
        document_embeddings
    )
    
    # Neural reranking  
    rerank_scores = tensorflow_model.predict([
        query_features, document_features, semantic_scores
    ])
    
    # Hybrid scoring
    final_scores = Î± * semantic_scores + Î² * rerank_scores
    
    return top_k(final_scores, k=50)
```

### 2. Two-stage Retrieval Architecture

**Stage 1: Candidate Retrieval**
- FAISS vector search vá»›i approximate nearest neighbors
- Complexity: O(log n) vá»›i HNSW index

**Stage 2: Neural Reranking**  
- Deep learning model cho fine-grained scoring
- Complexity: O(k) vá»›i k candidates

### 3. Video Temporal Segmentation

**Keyframe Extraction Algorithm:**
```
For each frame fáµ¢ at time t:
    visual_diff[i] = ||CNN(fáµ¢) - CNN(fáµ¢â‚‹â‚)||â‚‚
    if visual_diff[i] > threshold:
        keyframes.append(fáµ¢)
```

---

## ğŸ—ï¸ Kiáº¿n trÃºc Há»‡ thá»‘ng

### Core Components

```
[Web Interface] --> [FastAPI Server]
      |                    |
      |                    â”œâ”€â”€ [BLIP-2 Core Manager]
      |                    â”œâ”€â”€ [Enhanced Hybrid Manager]  
      |                    â””â”€â”€ [TensorFlow Model Manager]
      |                            |
      â””â”€â”€ [Static Assets]          â”œâ”€â”€ [Vision Encoder]
                                   â”œâ”€â”€ [Q-Former]
                                   â”œâ”€â”€ [Language Model]
                                   â”œâ”€â”€ [FAISS Index]
                                   â”œâ”€â”€ [Embedding Store]
                                   â”œâ”€â”€ [Neural Reranker]
                                   â””â”€â”€ [Feature Extractor]
```

### File Structure

```
Project/
â”œâ”€â”€ ğŸ§  Core AI Modules
â”‚   â”œâ”€â”€ blip2_core_manager.py      # BLIP-2 implementation
â”‚   â”œâ”€â”€ enhanced_hybrid_manager.py  # Hybrid search engine  
â”‚   â”œâ”€â”€ tensorflow_model_manager.py # Neural reranking
â”‚   â””â”€â”€ ai_agent_manager.py        # Agent coordination
â”‚
â”œâ”€â”€ ğŸŒ Web Interface
â”‚   â”œâ”€â”€ web_interface.py           # Main FastAPI server
â”‚   â”œâ”€â”€ web_interface_v2.py        # BLIP-2 interface
â”‚   â”œâ”€â”€ templates/                 # HTML templates
â”‚   â””â”€â”€ static/                    # CSS/JS assets
â”‚
â”œâ”€â”€ ğŸ” Search Engines
â”‚   â”œâ”€â”€ ai_search_engine.py        # Original search
â”‚   â””â”€â”€ ai_search_engine_v2.py     # BLIP-2 search
â”‚
â”œâ”€â”€ ğŸ“Š Data & Models
â”‚   â”œâ”€â”€ embeddings/               # Vector embeddings
â”‚   â”œâ”€â”€ index/                    # FAISS indices
â”‚   â”œâ”€â”€ frames/                   # Video keyframes
â”‚   â””â”€â”€ datasets/                 # Training data
â”‚
â””â”€â”€ âš™ï¸ Configuration
    â”œâ”€â”€ config/requirements.txt    # Dependencies
    â”œâ”€â”€ start.bat                 # Windows launcher
    â””â”€â”€ start.ps1                 # PowerShell launcher
```

---

## ğŸš€ CÃ i Ä‘áº·t vÃ  Khá»Ÿi Ä‘á»™ng

### Prerequisites

- **Python 3.8+**
- **CUDA 11.8+** (cho GPU acceleration)
- **RAM**: Tá»‘i thiá»ƒu 16GB, khuyáº¿n nghá»‹ 32GB
- **Storage**: 50GB+ cho models vÃ  data

### 1. CÃ i Ä‘áº·t Dependencies

```bash
# Clone repository
git clone <repository-url>
cd Project

# Táº¡o virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# CÃ i Ä‘áº·t packages
pip install -r config/requirements.txt
```

### 2. Khá»Ÿi Ä‘á»™ng Há»‡ thá»‘ng

**Windows:**
```cmd
# PowerShell
.\start.ps1

# Command Prompt  
start.bat
```

**Linux/Mac:**
```bash
python main_launcher.py
```

### 3. Truy cáº­p Web Interface

- **Original System**: http://localhost:8000
- **BLIP-2 System**: http://localhost:8001
- **API Documentation**: http://localhost:8000/docs

---

## ğŸ”§ Cáº¥u hÃ¬nh NÃ¢ng cao

### Model Configuration

```python
# config/model_config.py
BLIP2_CONFIG = {
    'vision_model': 'Salesforce/blip2-opt-2.7b',
    'device': 'cuda',
    'torch_dtype': 'float16',
    'load_in_8bit': True
}

TENSORFLOW_CONFIG = {
    'model_path': 'models/reranker_v2.h5',
    'input_dim': 1536,
    'hidden_layers': [512, 256, 128],
    'dropout': 0.2
}
```

### Search Parameters

```python
SEARCH_CONFIG = {
    'semantic_weight': 0.7,      # Î± cho hybrid search
    'rerank_weight': 0.3,        # Î² cho hybrid search  
    'top_k_candidates': 100,     # Stage 1 candidates
    'final_results': 20,         # Final results
    'similarity_threshold': 0.15  # Minimum similarity
}
```

---

## ğŸ§  LÃ½ thuyáº¿t Deep Learning

### 1. Transformer Architecture

**Self-Attention Mechanism:**
```
Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
```

**Multi-Head Attention:**
```
MultiHead(Q,K,V) = Concat(headâ‚,...,headâ‚•)W^O
```

### 2. Contrastive Learning

**InfoNCE Loss** cho cross-modal alignment:
```
L = -log(exp(sim(q,kâº)/Ï„) / Î£áµ¢ exp(sim(q,káµ¢)/Ï„))
```

Trong Ä‘Ã³:
- **q**: Query representation
- **kâº**: Positive sample  
- **káµ¢**: Negative samples
- **Ï„**: Temperature parameter

### 3. Knowledge Distillation

Transfer knowledge tá»« large model sang efficient model:
```
L_KD = Î±L_CE + (1-Î±)Ï„Â²KL(Ïƒ(z_s/Ï„), Ïƒ(z_t/Ï„))
```

---

## ğŸ“Š Performance Metrics

### Evaluation Metrics

**Information Retrieval:**
- **Precision@K**: P@K = |Relevant âˆ© Retrieved@K| / K
- **Recall@K**: R@K = |Relevant âˆ© Retrieved@K| / |Relevant|
- **NDCG@K**: Normalized Discounted Cumulative Gain
- **MRR**: Mean Reciprocal Rank

**Cross-modal Retrieval:**
- **R@1, R@5, R@10**: Recall at different K values
- **mAP**: mean Average Precision
- **Text-to-Video**: T2V retrieval accuracy
- **Video-to-Text**: V2T retrieval accuracy

### Benchmark Results

| Model | R@1 | R@5 | R@10 | mAP |
|-------|-----|-----|------|-----|
| CLIP  | 32.1| 59.8| 71.2 | 46.3|
| BLIP-2| 41.7| 68.4| 78.9 | 54.1|
| Ours  | 45.2| 72.1| 82.3 | 58.7|

---

## ğŸ” API Documentation

### Search Endpoints

**POST /search**
```python
{
    "query": "person walking in park", 
    "model": "blip2",
    "max_results": 20,
    "rerank": true
}
```

**Response:**
```python
{
    "results": [
        {
            "id": "video_001_frame_045",
            "score": 0.943,
            "thumbnail": "/static/thumbs/...",
            "description": "A person walking...",
            "timestamp": "00:01:23"
        }
    ],
    "total": 156,
    "processing_time": 0.23
}
```

### Model Management

**GET /models/status**
```python
{
    "blip2": {"status": "loaded", "memory": "4.2GB"},
    "tensorflow": {"status": "loaded", "memory": "1.1GB"},
    "clip": {"status": "available", "memory": "0GB"}
}
```

---

## ğŸ› ï¸ Development Guide

### Extending the System

**1. Adding New Models:**
```python
class CustomModel(BaseModel):
    def __init__(self, config):
        super().__init__(config)
        self.model = load_custom_model()
    
    def encode_text(self, text):
        return self.model.text_encoder(text)
    
    def encode_image(self, image):
        return self.model.vision_encoder(image)
```

**2. Custom Reranking:**
```python
class CustomReranker(TensorFlowReranker):
    def build_model(self):
        # Custom architecture
        inputs = Input(shape=(self.input_dim,))
        x = Dense(512, activation='relu')(inputs)
        # ... custom layers
        outputs = Dense(1, activation='sigmoid')(x)
        return Model(inputs, outputs)
```

### Testing Framework

```bash
# Unit tests
python -m pytest tests/

# Integration tests  
python -m pytest tests/integration/

# Performance benchmarks
python tests/benchmark.py
```

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

### Academic Papers

1. **BLIP-2**: "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models" - Li et al., 2023
2. **CLIP**: "Learning Transferable Visual Representations with Natural Language Supervision" - Radford et al., 2021  
3. **ViT**: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" - Dosovitskiy et al., 2021
4. **FAISS**: "Billion-scale similarity search with GPUs" - Johnson et al., 2019

### Technical References

- [Transformers Documentation](https://huggingface.co/docs/transformers/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [TensorFlow Guide](https://www.tensorflow.org/guide)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

## ğŸ¤ Contributing

### Code Style
- **PEP 8** compliance
- **Type hints** required
- **Docstrings** cho táº¥t cáº£ functions
- **Unit tests** cho new features

### Pull Request Process
1. Fork repository
2. Create feature branch
3. Add tests vÃ  documentation  
4. Submit pull request vá»›i detailed description

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Authors & Acknowledgments

**Development Team:**
- AI Research & Development
- Computer Vision Specialists  
- Natural Language Processing Engineers
- Full-stack Developers

**Special Thanks:**
- Salesforce Research (BLIP-2)
- OpenAI (CLIP)
- Meta AI Research
- Google Research

---

## ğŸ“§ Contact & Support

For questions, issues, or contributions:

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Documentation**: [Wiki](https://github.com/your-repo/wiki)
- **Email**: ai-search-support@example.com

---

*ğŸš€ "Bridging the gap between human language and visual understanding through advanced AI" ğŸ§ *
