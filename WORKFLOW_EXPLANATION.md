# ğŸ¯ **Flow LÃ m Viá»‡c Chi Tiáº¿t cá»§a Há»‡ Thá»‘ng AI Video Search**

## ğŸ—ï¸ **Kiáº¿n TrÃºc 3 Lá»›p vÃ  Workflow**

### **ğŸ“‹ Tá»•ng Quan Há»‡ Thá»‘ng**

```
ğŸ¤– Enhanced AI Video Search System (3-Layer Architecture)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸŒ LAYER 1: WEB INTERFACE                    â”‚
â”‚                     (4 Core Models - Frontend)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸš€ CLIP ViT Base  â”‚  ğŸ”¥ CLIP ViT Large  â”‚  ğŸŒ Chinese CLIP    â”‚
â”‚  ğŸ“ Sentence Transformers                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ğŸ”§ LAYER 2: TENSORFLOW BACKEND                   â”‚
â”‚                   (11 Specialized Models)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“± MobileNet V2   â”‚  ğŸ¯ Inception V3    â”‚  ğŸ—ï¸ ResNet 50       â”‚
â”‚  âš¡ EfficientNet   â”‚  ğŸŒ USE v4/Large    â”‚  ğŸ” SSD MobileNet    â”‚
â”‚  ğŸ¨ Faster R-CNN   â”‚  ğŸ“Š ImageNet        â”‚  ğŸ­ BiT ResNet      â”‚
â”‚  ğŸŒ USE Multilingual â”‚  ğŸ”§ Custom Models                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ğŸ¤– LAYER 3: AI AGENTS                           â”‚
â”‚                (2 Premium + 1 Local Agent)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¤– OpenAI GPT-4   â”‚  ğŸ§  Anthropic       â”‚  ğŸ  Local BLIP      â”‚
â”‚     Vision API     â”‚     Claude 3        â”‚     Models          â”‚
â”‚  (Requires API)    â”‚  (Requires API)     â”‚  (Free)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ **LAYER 1: Web Interface Flow (4 MÃ´ HÃ¬nh ChÃ­nh)**

### **ğŸ¯ MÃ´ HÃ¬nh NÃ o ÄÆ°á»£c Sá»­ Dá»¥ng Khi NÃ o**

#### **1. ğŸš€ CLIP ViT Base (Máº·c Äá»‹nh)**
```python
# File: web_interface.py
@app.on_event("startup")
async def startup_event():
    # Auto-load CLIP ViT Base lÃ m mÃ´ hÃ¬nh máº·c Ä‘á»‹nh
    success = model_manager.load_model("clip_vit_base")
    search_engine.set_active_model("vision_language", "clip_vit_base")
    
    # Build embeddings cho tÃ¬m kiáº¿m ngay láº­p tá»©c
    embeddings_success = search_engine.build_embeddings_index()
```

**Flow Hoáº¡t Äá»™ng:**
1. **Khá»Ÿi táº¡o**: Tá»± Ä‘á»™ng load khi start web interface
2. **Xá»­ lÃ½ query**: User nháº­p "person coding" â†’ CLIP encode text
3. **So sÃ¡nh**: TÃ­nh similarity vá»›i 3,801 frame embeddings
4. **Tráº£ káº¿t quáº£**: Top 10 frames vá»›i similarity scores

#### **2. ğŸ”¥ CLIP ViT Large (Cháº¥t LÆ°á»£ng Cao)**
```python
# Khi user chá»n model CLIP Large
@app.post("/api/models/switch")
async def switch_model(request: Request):
    if model_id == "clip_vit_large":
        # Load model má»›i
        success = model_manager.set_vision_language_model("CLIP ViT Large")
        
        # Rebuild toÃ n bá»™ embeddings vá»›i model má»›i
        search_engine.build_embeddings_index()  # 3,801 frames
```

**Flow Hoáº¡t Äá»™ng:**
1. **Switching**: User chá»n CLIP Large trong dropdown
2. **Reload Model**: Unload CLIP Base, load CLIP Large
3. **Rebuild Index**: Xá»­ lÃ½ láº¡i 3,801 frames (batch 32, GPU accelerated)
4. **Update Search**: Search engine sá»­ dá»¥ng embeddings má»›i

#### **3. ğŸŒ Chinese CLIP (Tá»‘i Æ¯u Tiáº¿ng Viá»‡t)**
```python
# Optimized cho Vietnamese queries
if model_id == "chinese_clip":
    success = model_manager.set_vision_language_model("Chinese CLIP")
    
# Xá»­ lÃ½ query tiáº¿ng Viá»‡t
query = "tÃ¬m ngÆ°á»i Ä‘Ã n Ã´ng"  # Vietnamese input
chinese_clip_embeddings = model.encode(query)  # Tá»‘i Æ°u cho tiáº¿ng Viá»‡t
```

**Flow Hoáº¡t Äá»™ng:**
1. **Language Detection**: Detect Vietnamese/Chinese text
2. **Optimized Encoding**: Chinese CLIP xá»­ lÃ½ tá»‘t hÆ¡n tiáº¿ng Viá»‡t
3. **Cross-Language**: Hiá»ƒu context vÄƒn hÃ³a Ã ÄÃ´ng
4. **Better Results**: Accuracy cao hÆ¡n cho Vietnamese queries

#### **4. ğŸ“ Sentence Transformers (Text-Only)**
```python
# Pure text similarity
if model_id == "sentence_transformers":
    text_embeddings = sentence_transformer.encode(query)
    # Chá»‰ so sÃ¡nh text metadata, khÃ´ng xá»­ lÃ½ hÃ¬nh áº£nh
```

**Flow Hoáº¡t Äá»™ng:**
1. **Text-Only**: Chá»‰ xá»­ lÃ½ text, bá» qua visual features
2. **Fast Processing**: KhÃ´ng cáº§n GPU cho image processing
3. **Metadata Search**: TÃ¬m kiáº¿m dá»±a trÃªn frame descriptions
4. **Ultra Fast**: Tá»‘c Ä‘á»™ nhanh nháº¥t trong 4 mÃ´ hÃ¬nh

---

## ğŸ”§ **LAYER 2: TensorFlow Backend Flow (11 MÃ´ HÃ¬nh)**

### **ğŸ­ CÃ¡ch 11 MÃ´ HÃ¬nh TensorFlow Hoáº¡t Äá»™ng**

#### **ğŸ¯ Image Feature Extraction Models (4 models)**

```python
# File: tensorflow_model_manager.py
class TensorFlowModelManager:
    def _setup_tensorflow_models(self):
        # 1. MobileNet V2 - Lightweight
        self.configs["mobilenet_v2"] = TensorFlowModelConfig(
            url="https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4",
            input_shape=(224, 224, 3),
            capabilities=["image_embedding", "feature_extraction"]
        )
        
        # 2. Inception V3 - Balanced
        self.configs["inception_v3"] = TensorFlowModelConfig(
            url="https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4",
            input_shape=(299, 299, 3),
            capabilities=["image_embedding", "feature_extraction"]
        )
        
        # 3. ResNet-50 - Deep Learning
        self.configs["resnet50"] = TensorFlowModelConfig(
            url="https://tfhub.dev/tensorflow/resnet_50/feature_vector/1",
            capabilities=["image_embedding", "feature_extraction"]
        )
        
        # 4. EfficientNet B0 - State-of-art
        self.configs["efficientnet_b0"] = TensorFlowModelConfig(
            url="https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1",
            capabilities=["image_embedding", "feature_extraction"]
        )
```

**Khi NÃ o ÄÆ°á»£c Sá»­ Dá»¥ng:**
- **Background Processing**: Cháº¡y song song vá»›i Layer 1
- **Advanced Analysis**: Khi cáº§n feature extraction chuyÃªn sÃ¢u
- **Batch Processing**: Xá»­ lÃ½ hÃ ng loáº¡t frames vá»›i different perspectives

#### **ğŸŒ Text Embedding Models (3 models)**

```python
# 5. Universal Sentence Encoder v4
self.configs["universal_sentence_encoder"] = TensorFlowModelConfig(
    url="https://tfhub.dev/google/universal-sentence-encoder/4",
    capabilities=["text_embedding", "semantic_search"]
)

# 6. USE Multilingual
self.configs["use_multilingual"] = TensorFlowModelConfig(
    url="https://tfhub.dev/google/universal-sentence-encoder-multilingual/3",
    capabilities=["text_embedding", "multilingual", "semantic_search"]
)

# 7. USE Large
self.configs["use_large"] = TensorFlowModelConfig(
    url="https://tfhub.dev/google/universal-sentence-encoder-large/5",
    capabilities=["text_embedding", "semantic_search", "high_quality"]
)
```

**Flow Hoáº¡t Äá»™ng:**
```python
# Backend text processing workflow
def encode_text_tensorflow(self, text: str, model_key: str = "universal_sentence_encoder"):
    # 1. Load TensorFlow model
    model = hub.load(config.url)
    
    # 2. Preprocess text
    processed_text = self._preprocess_text(text)
    
    # 3. Generate embeddings
    embeddings = model([processed_text])
    
    # 4. Return numpy array
    return embeddings.numpy()
```

#### **ğŸ” Object Detection Models (2 models)**

```python
# 8. SSD MobileNet - Fast Detection
self.configs["ssd_mobilenet"] = TensorFlowModelConfig(
    url="https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2",
    model_type="object_detection",
    input_shape=(320, 320, 3),
    capabilities=["object_detection", "real_time"]
)

# 9. Faster R-CNN - High Accuracy
self.configs["faster_rcnn"] = TensorFlowModelConfig(
    url="https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1",
    model_type="object_detection",
    input_shape=(640, 640, 3),
    capabilities=["object_detection", "high_accuracy"]
)
```

**Detection Workflow:**
```python
def detect_objects_tensorflow(self, image_path: str, model_key: str = "ssd_mobilenet"):
    # 1. Load detection model
    detector = hub.load(config.url)
    
    # 2. Preprocess image
    image = self.preprocess_image(image_path, config)
    
    # 3. Run detection
    detections = detector(image)
    
    # 4. Process results
    boxes = detections["detection_boxes"]
    classes = detections["detection_classes"]
    scores = detections["detection_scores"]
    
    return {
        "objects": processed_detections,
        "confidence_scores": scores,
        "bounding_boxes": boxes
    }
```

#### **ğŸ“Š Specialized Models (2 models)**

```python
# 10. ImageNet MobileNet - Classification
self.configs["imagenet_mobilenet"] = TensorFlowModelConfig(
    url="https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4",
    model_type="classification",
    capabilities=["classification", "imagenet"]
)

# 11. BiT ResNet-50 - Transfer Learning
self.configs["bit_resnet50"] = TensorFlowModelConfig(
    url="https://tfhub.dev/google/bit/s-r50x1/1",
    model_type="image_embedding",
    capabilities=["image_embedding", "transfer_learning"]
)
```

### **ğŸ”„ Backend Processing Workflow**

```python
# File: enhanced_hybrid_manager.py
class EnhancedHybridModelManager:
    def __init__(self):
        # Initialize TensorFlow Manager
        if TENSORFLOW_MODELS_AVAILABLE:
            self.tensorflow_manager = TensorFlowModelManager()
            
        # Load AI Agents
        if AI_AGENTS_AVAILABLE:
            self.ai_agent_manager = AIAgentManager()
    
    def process_with_tensorflow_backend(self, frame_path: str):
        """Background processing vá»›i 11 TensorFlow models"""
        results = {}
        
        # Feature extraction vá»›i multiple models
        for model_key in ["mobilenet_v2", "inception_v3", "resnet50"]:
            features = self.tensorflow_manager.extract_image_features(
                frame_path, model_key
            )
            results[model_key] = features
        
        # Object detection
        detections = self.tensorflow_manager.detect_objects(
            frame_path, "ssd_mobilenet"
        )
        results["detections"] = detections
        
        # Text analysis (if metadata available)
        if frame_metadata:
            text_embeddings = self.tensorflow_manager.encode_text(
                frame_metadata, "universal_sentence_encoder"
            )
            results["text_features"] = text_embeddings
        
        return results
```

---

## ğŸ¤– **LAYER 3: AI Agents Flow (2 Premium + 1 Local)**

### **ğŸ”‘ API Keys Integration Workflow**

#### **1. ğŸ¤– OpenAI GPT-4 Vision**

```python
# File: ai_agent_manager.py
class AIAgentManager:
    def analyze_frame(self, image_path: str, agent_name: str = "gpt4_vision"):
        if config.provider == "openai" and "vision" in config.capabilities:
            # Convert image to base64
            with open(image_path, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            
            # Call GPT-4 Vision API
            response = agent.chat.completions.create(
                model="gpt-4-vision-preview",
                messages=[
                    {
                        "role": "system", 
                        "content": "You are an expert video frame analyzer"
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Describe this video frame in detail"},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                        ]
                    }
                ]
            )
            
            return response.choices[0].message.content
```

**Khi NÃ o ÄÆ°á»£c Sá»­ Dá»¥ng:**
- **Professional Analysis**: Video analysis chuyÃªn nghiá»‡p
- **Detailed Descriptions**: MÃ´ táº£ chi tiáº¿t frame content
- **Context Understanding**: Hiá»ƒu context phá»©c táº¡p

#### **2. ğŸ§  Anthropic Claude 3**

```python
def generate_search_query(self, user_query: str, agent_name: str = "claude3"):
    if config.provider == "anthropic":
        response = agent.messages.create(
            model="claude-3-sonnet-20240229",
            messages=[
                {
                    "role": "user",
                    "content": f"Optimize this search query for video content: {user_query}"
                }
            ]
        )
        
        return response.content[0].text
```

**Khi NÃ o ÄÆ°á»£c Sá»­ Dá»¥ng:**
- **Query Optimization**: Tá»‘i Æ°u hÃ³a search queries
- **Reasoning**: PhÃ¢n tÃ­ch logic phá»©c táº¡p
- **Smart Suggestions**: Gá»£i Ã½ thÃ´ng minh

#### **3. ğŸ  Local BLIP Models (Free)**

```python
def analyze_frame_local(self, image_path: str):
    # Local BLIP model - no API key needed
    if "blip" in config.model.lower():
        processor = BlipProcessor.from_pretrained(config.model)
        model = BlipForConditionalGeneration.from_pretrained(config.model)
        
        # Process image
        image = Image.open(image_path).convert('RGB')
        inputs = processor(image, return_tensors="pt")
        
        # Generate caption
        out = model.generate(**inputs, max_length=50)
        caption = processor.decode(out[0], skip_special_tokens=True)
        
        return {"caption": caption, "confidence": "local_model"}
```

**Khi NÃ o ÄÆ°á»£c Sá»­ Dá»¥ng:**
- **Free Alternative**: KhÃ´ng cáº§n API keys
- **Basic Captioning**: MÃ´ táº£ cÆ¡ báº£n frames
- **Offline Processing**: Hoáº¡t Ä‘á»™ng offline

---

## ğŸš€ **Complete Workflow Example**

### **ğŸ“ User Search: "tÃ¬m ngÆ°á»i Ä‘Ã n Ã´ng"**

```
1. ğŸŒ LAYER 1: Web Interface
   â”œâ”€â”€ User types "tÃ¬m ngÆ°á»i Ä‘Ã n Ã´ng"
   â”œâ”€â”€ Chinese CLIP model detects Vietnamese
   â”œâ”€â”€ Encode query to 512D vector
   â””â”€â”€ Search 3,801 frame embeddings
   
2. ğŸ”§ LAYER 2: TensorFlow Backend (Parallel)
   â”œâ”€â”€ MobileNet V2: Extract lightweight features
   â”œâ”€â”€ ResNet-50: Deep feature analysis
   â”œâ”€â”€ USE Multilingual: Multilingual text understanding
   â””â”€â”€ SSD MobileNet: Detect objects in frames
   
3. ğŸ¤– LAYER 3: AI Agents (If API keys available)
   â”œâ”€â”€ Claude 3: "Optimize query for male person detection"
   â”œâ”€â”€ GPT-4 Vision: Analyze top result frames
   â””â”€â”€ Local BLIP: Generate captions for context

4. ğŸ“Š Result Fusion
   â”œâ”€â”€ Combine Layer 1 similarity scores (25.4%)
   â”œâ”€â”€ Enhance with Layer 2 object detection
   â”œâ”€â”€ Add Layer 3 intelligent descriptions
   â””â”€â”€ Return enriched results to user
```

### **âš¡ Performance Characteristics**

| Layer | Processing Time | Memory Usage | GPU Usage | Accuracy |
|-------|----------------|--------------|-----------|----------|
| **Layer 1** | 0.1-0.3s | 2-4 GB | High | Good-Excellent |
| **Layer 2** | 0.5-2.0s | 1-8 GB | Medium | Specialized |
| **Layer 3** | 1-5s | Low | None | Excellent |

### **ğŸ¯ Model Selection Strategy**

```python
def select_optimal_models(query_type: str, performance_priority: str):
    if query_type == "vietnamese_text":
        primary = "chinese_clip"              # Layer 1
        secondary = "use_multilingual"        # Layer 2
        enhancement = "claude3"               # Layer 3
        
    elif query_type == "fast_search":
        primary = "clip_vit_base"            # Layer 1
        secondary = "mobilenet_v2"           # Layer 2  
        enhancement = "blip_local"           # Layer 3
        
    elif query_type == "high_accuracy":
        primary = "clip_vit_large"           # Layer 1
        secondary = ["resnet50", "inception_v3"]  # Layer 2
        enhancement = "gpt4_vision"          # Layer 3
        
    return {
        "layer1": primary,
        "layer2": secondary,
        "layer3": enhancement
    }
```

---

## ğŸ“‹ **TÃ³m Táº¯t Flow Hoáº¡t Äá»™ng**

### **ğŸ¯ 4 MÃ´ HÃ¬nh ChÃ­nh (Layer 1)**
- **Always Active**: LuÃ´n sáºµn sÃ ng cho user interaction
- **Real-time**: Xá»­ lÃ½ search queries ngay láº­p tá»©c
- **User Choice**: User chá»n model qua dropdown
- **Primary Results**: Cung cáº¥p káº¿t quáº£ chÃ­nh

### **ğŸ”§ 11 MÃ´ HÃ¬nh TensorFlow (Layer 2)**  
- **Background**: Cháº¡y ngáº§m Ä‘á»ƒ enhance results
- **Specialized**: Má»—i model cÃ³ chuyÃªn mÃ´n riÃªng
- **Parallel Processing**: Xá»­ lÃ½ song song nhiá»u models
- **Feature Enhancement**: Bá»• sung thÃ´ng tin chuyÃªn sÃ¢u

### **ğŸ¤– AI Agents (Layer 3)**
- **Premium Enhancement**: NÃ¢ng cao vá»›i API keys
- **Intelligent Analysis**: PhÃ¢n tÃ­ch thÃ´ng minh
- **Context Understanding**: Hiá»ƒu context phá»©c táº¡p
- **Optional**: Hoáº¡t Ä‘á»™ng mÃ  khÃ´ng cáº§n API keys (Local BLIP)

**Káº¿t quáº£**: Há»‡ thá»‘ng 3 lá»›p hoáº¡t Ä‘á»™ng song song, bá»• sung cho nhau táº¡o ra tráº£i nghiá»‡m search thÃ´ng minh vÃ  chÃ­nh xÃ¡c!
